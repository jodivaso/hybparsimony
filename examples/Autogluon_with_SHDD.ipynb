{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import log_loss\n",
    "from hybparsimony import HYBparsimony\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from hybparsimony import util\n",
    "import openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0: Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edmans/anaconda3/envs/hyb/lib/python3.10/site-packages/openml/datasets/functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9822, 85)\n"
     ]
    }
   ],
   "source": [
    "# Get COIL2000 dataset\n",
    "dataset = openml.datasets.get_dataset('COIL2000')\n",
    "label = dataset.default_target_attribute\n",
    "X_orig, y_orig, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=label)\n",
    "input_names = X_orig.columns\n",
    "print(X_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 50% for train/validation and 50% for testing\n",
    "train_data, test_data, y_train, y_test = train_test_split(X_orig, \n",
    "                                                        y_orig, \n",
    "                                                        test_size=0.50, \n",
    "                                                        shuffle=True, \n",
    "                                                        random_state=0)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use Autogluon with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230921_111451/\"\n",
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230921_111451/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 10:33:52 UTC 2\n",
      "Disk Space Avail:   1167.83 GB / 1574.05 GB (74.2%)\n",
      "Train Data Rows:    4911\n",
      "Train Data Columns: 85\n",
      "Label Column: CARAVAN\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    127435.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.42 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['AZEILPL']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['AZEILPL']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 84 | ['MOSTYPE', 'MAANTHUI', 'MGEMOMV', 'MGEMLEEF', 'MOSHOOFD', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', [])       : 79 | ['MOSTYPE', 'MAANTHUI', 'MGEMOMV', 'MGEMLEEF', 'MOSHOOFD', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['PFIETS', 'AWALAND', 'APERSONG', 'AGEZONG', 'AINBOED']\n",
      "\t0.7s = Fit runtime\n",
      "\t84 features in original data used to generate 84 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.41 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.10181225819588678, Train Rows: 4411, Val Rows: 500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 149.21s of the 149.21s of remaining time.\n",
      "\t-0.7605\t = Validation score   (-log_loss)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 149.07s of the 149.07s of remaining time.\n",
      "\t-0.8523\t = Validation score   (-log_loss)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 148.97s of the 148.96s of remaining time.\n",
      "\t-0.2111\t = Validation score   (-log_loss)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 145.8s of the 145.8s of remaining time.\n",
      "\t-0.2213\t = Validation score   (-log_loss)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 144.44s of the 144.44s of remaining time.\n",
      "\t-0.2874\t = Validation score   (-log_loss)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 142.85s of the 142.85s of remaining time.\n",
      "\t-0.2444\t = Validation score   (-log_loss)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 141.23s of the 141.22s of remaining time.\n",
      "\t-0.2029\t = Validation score   (-log_loss)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 140.41s of the 140.41s of remaining time.\n",
      "\t-0.2793\t = Validation score   (-log_loss)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 138.55s of the 138.55s of remaining time.\n",
      "\t-0.2612\t = Validation score   (-log_loss)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 136.59s of the 136.59s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-0.246\t = Validation score   (-log_loss)\n",
      "\t4.4s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 132.09s of the 132.09s of remaining time.\n",
      "\t-0.2177\t = Validation score   (-log_loss)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 131.62s of the 131.61s of remaining time.\n",
      "\t-0.2112\t = Validation score   (-log_loss)\n",
      "\t3.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 127.8s of the 127.8s of remaining time.\n",
      "\t-0.2225\t = Validation score   (-log_loss)\n",
      "\t4.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.21s of the 123.27s of remaining time.\n",
      "\t-0.2029\t = Validation score   (-log_loss)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 27.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230921_111451/\")\n"
     ]
    }
   ],
   "source": [
    "time_autogluon = 150 # in seconds\n",
    "\n",
    "# Train with features\n",
    "train_data[label] = y_train.values\n",
    "predictor = TabularPredictor(label=label, eval_metric='log_loss').fit(train_data, time_limit=time_autogluon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_loss with test using all the features=0.19725106895041525\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# Shows performance with a new dataset\n",
    "y_pred = predictor.predict_proba(test_data)\n",
    "Log_loss_all = log_loss(y_true=y_test.values, y_pred=y_pred)\n",
    "print(f'Log_loss with test using all the features={Log_loss_all}')\n",
    "print('##################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Search the best features with HYB-PARSIMONY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_custom(cromosoma, **kwargs):\n",
    "    global label\n",
    "\n",
    "    X_train = kwargs[\"X\"]\n",
    "    y_train = kwargs[\"y\"]\n",
    "        \n",
    "    # Extract features from the original DB plus response (last column)\n",
    "    X_fs_selec = X_train.loc[: , cromosoma.columns]\n",
    "    # Get 20% for validation\n",
    "    x_train_custom, x_test_custom, y_train_custom, y_test_custom = train_test_split(X_fs_selec, \n",
    "                                                                                    y_train, \n",
    "                                                                                    test_size=0.20, \n",
    "                                                                                    shuffle=True, \n",
    "                                                                                    random_state=0)\n",
    "    X_train_df = pd.DataFrame(np.hstack([x_train_custom, y_train_custom.reshape(-1,1).astype(int)]))\n",
    "    X_train_df.columns = list(X_fs_selec.columns)+[label]\n",
    "    X_test_df = pd.DataFrame(x_test_custom)\n",
    "    predictor = TabularPredictor(label=label, eval_metric='log_loss', verbosity=0).fit(X_train_df, time_limit=time_autogluon)\n",
    "    y_pred = predictor.predict_proba(X_test_df)\n",
    "    fitness_val = -log_loss(y_true=y_test_custom, y_pred=y_pred)\n",
    "    return np.array([fitness_val, np.sum(cromosoma.columns)]), predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 0\n",
      "Best model -> Score = -0.211364 Complexity = 76.0 \n",
      "Iter = 0 -> MeanVal = -0.21634  ValBest = -0.211364   ComplexBest = 76.0 Time(min) = 7.386587\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.211364 Complexity = 76.0 \n",
      "Iter = 1 -> MeanVal = -0.218023  ValBest = -0.212938   ComplexBest = 73.0 Time(min) = 7.295187\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 2 -> MeanVal = -0.218252  ValBest = -0.209308   ComplexBest = 52.0 Time(min) = 7.921135\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 3 -> MeanVal = -0.217136  ValBest = -0.212988   ComplexBest = 41.0 Time(min) = 7.63907\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 4 -> MeanVal = -0.215803  ValBest = -0.211512   ComplexBest = 49.0 Time(min) = 8.471128\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 5 -> MeanVal = -0.217571  ValBest = -0.213319   ComplexBest = 48.0 Time(min) = 7.746143\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 6 -> MeanVal = -0.217578  ValBest = -0.213328   ComplexBest = 54.0 Time(min) = 6.392602\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 7 -> MeanVal = -0.217584  ValBest = -0.213067   ComplexBest = 52.0 Time(min) = 6.206112\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 8 -> MeanVal = -0.217016  ValBest = -0.211523   ComplexBest = 50.0 Time(min) = 5.784674\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 9 -> MeanVal = -0.21688  ValBest = -0.211844   ComplexBest = 49.0 Time(min) = 5.891965\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 10 -> MeanVal = -0.216198  ValBest = -0.210499   ComplexBest = 49.0 Time(min) = 5.99039\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 11 -> MeanVal = -0.217312  ValBest = -0.211712   ComplexBest = 47.0 Time(min) = 5.806943\n",
      "\n",
      "Running iteration 12\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 12 -> MeanVal = -0.216232  ValBest = -0.212163   ComplexBest = 53.0 Time(min) = 6.538658\n",
      "\n",
      "Running iteration 13\n",
      "Best model -> Score = -0.209308 Complexity = 52.0 \n",
      "Iter = 13 -> MeanVal = -0.216191  ValBest = -0.209471   ComplexBest = 50.0 Time(min) = 6.587304\n",
      "\n",
      "Running iteration 14\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 14 -> MeanVal = -0.214639  ValBest = -0.207278   ComplexBest = 43.0 Time(min) = 5.921523\n",
      "\n",
      "Running iteration 15\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 15 -> MeanVal = -0.217162  ValBest = -0.212442   ComplexBest = 38.0 Time(min) = 5.783928\n",
      "\n",
      "Running iteration 16\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 16 -> MeanVal = -0.216343  ValBest = -0.208279   ComplexBest = 47.0 Time(min) = 5.687106\n",
      "\n",
      "Running iteration 17\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 17 -> MeanVal = -0.215691  ValBest = -0.209735   ComplexBest = 39.0 Time(min) = 5.89927\n",
      "\n",
      "Running iteration 18\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 18 -> MeanVal = -0.214856  ValBest = -0.208978   ComplexBest = 44.0 Time(min) = 5.864432\n",
      "\n",
      "Running iteration 19\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 19 -> MeanVal = -0.215083  ValBest = -0.208671   ComplexBest = 45.0 Time(min) = 5.759666\n",
      "\n",
      "Running iteration 20\n",
      "Best model -> Score = -0.207278 Complexity = 43.0 \n",
      "Iter = 20 -> MeanVal = -0.214466  ValBest = -0.209044   ComplexBest = 41.0 Time(min) = 6.191359\n",
      "\n",
      "Running iteration 21\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 21 -> MeanVal = -0.213463   ValBest = -0.20586   ComplexBest = 44.0 Time(min) = 6.190824\n",
      "\n",
      "Running iteration 22\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 22 -> MeanVal = -0.21359  ValBest = -0.208327   ComplexBest = 43.0 Time(min) = 5.895569\n",
      "\n",
      "Running iteration 23\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 23 -> MeanVal = -0.213614  ValBest = -0.207359   ComplexBest = 42.0 Time(min) = 6.952709\n",
      "\n",
      "Running iteration 24\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 24 -> MeanVal = -0.21441   ValBest = -0.20706   ComplexBest = 42.0 Time(min) = 6.149906\n",
      "\n",
      "Running iteration 25\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 25 -> MeanVal = -0.213739  ValBest = -0.207117   ComplexBest = 44.0 Time(min) = 6.296593\n",
      "\n",
      "Running iteration 26\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 26 -> MeanVal = -0.214136  ValBest = -0.208817   ComplexBest = 42.0 Time(min) = 5.854235\n",
      "\n",
      "Running iteration 27\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 27 -> MeanVal = -0.214313  ValBest = -0.208148   ComplexBest = 40.0 Time(min) = 6.150687\n",
      "\n",
      "Running iteration 28\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 28 -> MeanVal = -0.214068  ValBest = -0.208089   ComplexBest = 43.0 Time(min) = 5.969071\n",
      "\n",
      "Running iteration 29\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 29 -> MeanVal = -0.215228  ValBest = -0.209216   ComplexBest = 45.0 Time(min) = 5.967605\n",
      "\n",
      "Running iteration 30\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 30 -> MeanVal = -0.21516  ValBest = -0.208068   ComplexBest = 43.0 Time(min) = 5.811961\n",
      "\n",
      "Running iteration 31\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 31 -> MeanVal = -0.212581  ValBest = -0.207072   ComplexBest = 38.0 Time(min) = 6.219733\n",
      "\n",
      "Running iteration 32\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 32 -> MeanVal = -0.214142  ValBest = -0.210958   ComplexBest = 39.0 Time(min) = 5.688024\n",
      "\n",
      "Running iteration 33\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 33 -> MeanVal = -0.21392  ValBest = -0.209784   ComplexBest = 39.0 Time(min) = 5.644269\n",
      "\n",
      "Running iteration 34\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 34 -> MeanVal = -0.214439  ValBest = -0.210883   ComplexBest = 40.0 Time(min) = 5.789446\n",
      "\n",
      "Running iteration 35\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 35 -> MeanVal = -0.214551  ValBest = -0.207117   ComplexBest = 44.0 Time(min) = 6.412789\n",
      "\n",
      "Running iteration 36\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 36 -> MeanVal = -0.214197  ValBest = -0.209422   ComplexBest = 42.0 Time(min) = 5.956857\n",
      "\n",
      "Running iteration 37\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 37 -> MeanVal = -0.213734  ValBest = -0.206111   ComplexBest = 41.0 Time(min) = 5.833931\n",
      "\n",
      "Running iteration 38\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 38 -> MeanVal = -0.215525  ValBest = -0.207929   ComplexBest = 40.0 Time(min) = 5.702271\n",
      "\n",
      "Running iteration 39\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 39 -> MeanVal = -0.215344  ValBest = -0.210025   ComplexBest = 43.0 Time(min) = 5.697854\n",
      "\n",
      "Running iteration 40\n",
      "Best model -> Score = -0.20586 Complexity = 44.0 \n",
      "Iter = 40 -> MeanVal = -0.213437  ValBest = -0.208561   ComplexBest = 41.0 Time(min) = 5.605442\n",
      "\n",
      "Early stopping reached. Stopped.\n",
      "Selected feats with HYB-PARSIMONY num=44:['MAANTHUI' 'MGEMOMV' 'MGEMLEEF' 'MGODPR' 'MGODOV' 'MGODGE' 'MRELSA'\n",
      " 'MFALLEEN' 'MFWEKIND' 'MBERZELF' 'MBERBOER' 'MBERARBO' 'MSKB1' 'MSKB2'\n",
      " 'MSKC' 'MZFONDS' 'MZPART' 'MINKM30' 'MINKGEM' 'PWAPART' 'PWALAND'\n",
      " 'PPERSAUT' 'PBESAUT' 'PVRAAUT' 'PAANHANG' 'PTRACTOR' 'PGEZONG' 'PBRAND'\n",
      " 'PZEILPL' 'PINBOED' 'PBYSTAND' 'AWAPART' 'AWABEDR' 'AWALAND' 'APERSAUT'\n",
      " 'AMOTSCO' 'ATRACTOR' 'ABROM' 'APERSONG' 'ABRAND' 'AZEILPL' 'APLEZIER'\n",
      " 'AINBOED' 'ABYSTAND']\n",
      "######################################################\n"
     ]
    }
   ],
   "source": [
    "HYBparsimony_model = HYBparsimony(fitness=fitness_custom,\n",
    "                                features=input_names,\n",
    "                                rerank_error=0.001,\n",
    "                                gamma_crossover=0.50,\n",
    "                                seed_ini=0,\n",
    "                                npart=15,\n",
    "                                maxiter=100,\n",
    "                                early_stop=20,\n",
    "                                verbose=1,\n",
    "                                n_jobs=1)\n",
    "HYBparsimony_model.fit(train_data[input_names], train_data[label].values)\n",
    "best_model_probsfeats = HYBparsimony_model.best_model_conf[-len(input_names):]\n",
    "selec_feats = np.array(input_names)[best_model_probsfeats>=0.50]\n",
    "print(f'Selected feats with HYB-PARSIMONY num={len(selec_feats)}:{selec_feats}')\n",
    "print('######################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3: Use Autogluon with the Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230921_153153/\"\n",
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230921_153153/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 10:33:52 UTC 2\n",
      "Disk Space Avail:   1124.67 GB / 1574.05 GB (71.5%)\n",
      "Train Data Rows:    4911\n",
      "Train Data Columns: 44\n",
      "Label Column: CARAVAN\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    127189.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['AZEILPL']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['AZEILPL']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 43 | ['MAANTHUI', 'MGEMOMV', 'MGEMLEEF', 'MGODPR', 'MGODOV', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', [])       : 40 | ['MAANTHUI', 'MGEMOMV', 'MGEMLEEF', 'MGODPR', 'MGODOV', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['AWALAND', 'APERSONG', 'AINBOED']\n",
      "\t0.2s = Fit runtime\n",
      "\t43 features in original data used to generate 43 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.10181225819588678, Train Rows: 4411, Val Rows: 500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 149.73s of the 149.73s of remaining time.\n",
      "\t-0.727\t = Validation score   (-log_loss)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 149.6s of the 149.59s of remaining time.\n",
      "\t-0.7853\t = Validation score   (-log_loss)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 149.53s of the 149.52s of remaining time.\n",
      "\t-0.2055\t = Validation score   (-log_loss)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 148.01s of the 148.01s of remaining time.\n",
      "\t-0.2062\t = Validation score   (-log_loss)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 146.79s of the 146.78s of remaining time.\n",
      "\t-0.2495\t = Validation score   (-log_loss)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 145.37s of the 145.37s of remaining time.\n",
      "\t-0.2724\t = Validation score   (-log_loss)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.94s of the 143.94s of remaining time.\n",
      "\t-0.1993\t = Validation score   (-log_loss)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 143.15s of the 143.14s of remaining time.\n",
      "\t-0.2473\t = Validation score   (-log_loss)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 141.78s of the 141.78s of remaining time.\n",
      "\t-0.2279\t = Validation score   (-log_loss)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 140.07s of the 140.07s of remaining time.\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-0.2201\t = Validation score   (-log_loss)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 136.21s of the 136.2s of remaining time.\n",
      "\t-0.2009\t = Validation score   (-log_loss)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 135.82s of the 135.81s of remaining time.\n",
      "\t-0.2034\t = Validation score   (-log_loss)\n",
      "\t5.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 130.43s of the 130.43s of remaining time.\n",
      "\t-0.2183\t = Validation score   (-log_loss)\n",
      "\t4.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.73s of the 126.29s of remaining time.\n",
      "\t-0.197\t = Validation score   (-log_loss)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 24.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230921_153153/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, eval_metric='log_loss').fit(train_data[list(selec_feats)+[label]], \n",
    "                                                                      time_limit=time_autogluon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_loss with test using the selected features=0.1975637808404383\n",
      "Difference between all features=85 and the selected feats=44 diff=41\n",
      "Difference between log_loss with all features=0.19725106895041525 and with the selected feats=0.1975637808404383 diff=-0.00031271189002304856\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict_proba(test_data[selec_feats])\n",
    "Log_loss_selected = log_loss(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(f'Log_loss with test using the selected features={Log_loss_selected}')\n",
    "print(f'Difference between all features={len(input_names)} and the selected feats={len(selec_feats)} diff={len(input_names)-len(selec_feats)}')\n",
    "print(f'Difference between log_loss with all features={Log_loss_all} and with the selected feats={Log_loss_selected} diff={Log_loss_all-Log_loss_selected}')\n",
    "print('#########################################################################')  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes, HYB-PARSIMONY is such an intensive search method that when working with SHDD the method may find parsimonious solutions that are too specific to that set of instances. Sometimes, the selected features may be the most appropriate for that sample but not be sufficient to create a model that will generalize correctly in the future. To reduce this over-fitting and to find a feature selection that can be used to create a robust model that generalizes correctly, we propose the following methodology (if time and resources are available):\n",
    "\n",
    "1. Repeat $n$ runs, with different random seeds, the search for the best model with HYB-PARSIMONY and hold-out validation. In each repetition, extract the feature probability vector of the best individual (*best_model_probsfeats*).\n",
    "2. Average the probabilities for each feature and select those that have a value greater than a given threshold, $thr_{fs}$.\n",
    "3. Train Autogluon with the selected features.\n",
    "4. Repeat points 2 and 3 with different $thr_{fs}$.\n",
    "5. Select the model that obtains the best error validation $J$ or with another test dataset.\n",
    "\n",
    "More info see: \n",
    "\n",
    "Divason, J., Pernia-Espinoza, A., Romero, A., Martinez-de-Pison, F.J. (2023). [Hybrid Intelligent Parsimony Search in Small High-Dimensional Datasets.](https://link.springer.com/content/pdf/10.1007/978-3-031-40725-3_33.pdf?pdf=inline%20link) In: Garcia Bringas, P., et al. Hybrid Artificial Intelligent Systems. HAIS 2023. Lecture Notes in Computer Science(), vol 14001. Springer, Cham. https://doi.org/10.1007/978-3-031-40725-3_33.\n",
    "\n",
    "\n",
    "\n",
    "Bibtex ref:\n",
    "\n",
    "@InProceedings{10.1007/978-3-031-40725-3_33,\n",
    "author=\"Divas{\\'o}n, Jose\n",
    "and Pernia-Espinoza, Alpha\n",
    "and Romero, Ana\n",
    "and Martinez-de-Pison, Francisco Javier\",\n",
    "editor=\"Garc{\\'i}a Bringas, Pablo\n",
    "and P{\\'e}rez Garc{\\'i}a, Hilde\n",
    "and Mart{\\'i}nez de Pis{\\'o}n, Francisco Javier\n",
    "and Mart{\\'i}nez {\\'A}lvarez, Francisco\n",
    "and Troncoso Lora, Alicia\n",
    "and Herrero, {\\'A}lvaro\n",
    "and Calvo Rolle, Jos{\\'e} Luis\n",
    "and Quinti{\\'a}n, H{\\'e}ctor\n",
    "and Corchado, Emilio\",\n",
    "title=\"Hybrid Intelligent Parsimony Search in Small High-Dimensional Datasets\",\n",
    "booktitle=\"Hybrid Artificial Intelligent Systems\",\n",
    "year=\"2023\",\n",
    "publisher=\"Springer Nature Switzerland\",\n",
    "address=\"Cham\",\n",
    "pages=\"384-396\",\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
